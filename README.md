# Face-Emotion-Detection
Problem Statement
Emotion detection has been around for ages, taking a step forward, human emotion displayed by face and felt by brain captured in other video electric signal or image form can be approximated. Human emotion detection is the need of the hour so that modern artificial intelligence systems can emulate and gauge reactions from face. This can be helpful to make informed decision be it regarding identification of intent, promotion of offers or security related threats.

It can also be used in different fields,  for e.g. Retailers may use these metrics to evaluate customer interest. Healthcare providers can provide better service by using additional information about patients' emotional state during treatment. Entertainment producers can monitor audience engagement in events to consistently create desired content.

Humans are well-trained in reading the emotions of others, in fact, at just 14 months old, babies can already tell the difference between happy and sad. But can computers do a better job than us in accessing emotional states?

So our objective in this project is to train a deep learning model which can detect the emotion of face.

Proposed Solutions
To build this system, we implemented a “YOLOv5”

For labeling the Image, Label Image is used.

And for live demo we have used OpenCV

Data Summary!
Our dataset is a collection of  captured Images of different emotions. It contains a total of around 300 face annotations, where images are also of various resolution. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Only color images are used. 

Important Dependencies

Pytorch (Torch Vision and Torch)
Yolov5
OpenCV
Git
UUID (For Unique Id)
LabelImg (Custom Annotation Tool)

Final Demo!
![image](https://user-images.githubusercontent.com/108734660/216535723-6806ea18-d706-4786-ac4e-21e31e901478.png)

